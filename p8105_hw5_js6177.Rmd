---
title: "p8105_hw5_js6177"
author: "Jiayi Shi"
date: "2022-11-03"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(readxl)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
set.seed(1)
```

## Problem 1

```{r message=FALSE}
path = "data/data"

study_df = 
  tibble(
    file_name = list.files(path)
  ) %>% 
  mutate(
    obs = map(str_c(path, "/", file_name), read_csv),
    file_name = tools::file_path_sans_ext(file_name)
    ) %>% 
  unnest(obs) %>% 
  separate(file_name, into = c("arm","id"), sep = "_") %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  mutate(
    week = as.integer(week)
  ) %>% 
  select(arm, id, week, observation)
```

plot:

```{r}
study_df %>% 
  ggplot(aes(x = week, y = observation, color = id))+
  geom_path()
```

numeric analysis

```{r}
study_df %>% 
  group_by(arm) %>% 
  summarise(
    obs_mean = mean(observation),
    obs_sd = sd(observation),
    obs_max = max(observation),
    obs_min = min(observation)
  )
```

## Problem 2

Read raw data:

```{r message=FALSE}
homicide_data = read_csv("data/homicide-data.csv")
```

The raw data set has `r nrow(homicide_data)` homicides in 50 large US cities from 2007 to 2017. It contains `r ncol(homicide_data)` variables, including the id of the killing, the reported date, the basic demographic information about each victim (first name, last name, race, age and sex), the location of the killing (city, state, latitude and longitude) and whether an arrest was made.

The code below createa a `city_state` variable and summarizes within cities to obtain the total number of homicides and the number of unsolved homicides. The result is solved in `table`.

```{r}
homicide_data =
  mutate(homicide_data, city_state = str_c(city, state, sep = ", "))

table = homicide_data %>% 
  group_by(city_state) %>% 
  summarise(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest","Open/No arrest"))
    )

table %>% knitr::kable()
```


```{r}
unsolved = table %>% 
  filter(city_state == "Baltimore, MD") %>% 
  pull(unsolved)

total = table %>% 
  filter(city_state == "Baltimore, MD") %>%
  pull(total)

prop = prop.test(unsolved, total)
```

* estimated proportion:

```{r}
broom::tidy(prop) %>% pull(estimate) %>% round(3)
```

* confidence interval:

```{r}
lower_bound = broom::tidy(prop) %>% pull(conf.low) %>% round(3)
upper_bound = broom::tidy(prop) %>% pull(conf.high) %>% round(3)

str_c(lower_bound, upper_bound, sep = ",")
```

I define a function `prop_ci` to derive the estimated proportion and confidence intervals with input. 
```{r}
prop_ci = function(data){
  
  prop = prop.test(data$unsolved, data$total)
  
  broom::tidy(prop) %>% 
    select(estimate, conf.low, conf.high)
}
```

I use list columns to get estimated proportions and CIs for each city. I first nest the `total` and `unsolved` columns in `table` and then map it to the function I defined above.

By the warning message, I check the data for Tulsa, AL, which only has one killing in total and zero unsolved case, so the Chi-squared approximation may be incorrect. Therefore, I mutate the confidence interval for this specific city accordingly.

```{r}
prop_df =
  nest(table, data = total:unsolved) %>% 
  mutate(unsolved_data = map(.$data, prop_ci)) %>%
  unnest(unsolved_data) %>% 
  select(-data) %>% 
  rename(conf_low = conf.low, conf_high = conf.high) %>% 
  mutate(conf_high = replace(conf_high, city_state == "Tulsa, AL", 0))
```

The code below create a plot that shows the estimates and CIs for each city.

```{r}
prop_df %>% 
  # Organize cities according to the proportion of unsolved homicide.
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  
  ggplot(aes(x = city_state, y = estimate))+
  geom_point()+
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high))+
  labs(title = "Proportion of unsolved homicides for 50 U.S. cities", 
       x = "Proportion of unsolved homicides", 
       y = "City", 
       caption = "Error bars represent 95% confidence interval") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.2, hjust = 1))
```

## Problem 3

I write a function `sim_mu_p` to generate a dataset which follows a normal distribution with mean equals 0 and standard deviation equals 5, and then obtain the estimated mu and p-value by performing a t test at 0.05 significance level.

```{r}
n = 30
sigma = 5
mu = 0

sim_mu_p = function(mu){
  
  sim_data = tibble(x = rnorm(n, mu, sigma))
  
  sim_data %>% 
  t.test(mu = 0, conf.level = 0.95) %>% 
  broom::tidy() %>% 
  select(estimate, p.value)
}
```

Generate 5000 datasets and apply the function defined above.

```{r}
output = vector("list", 5000)

for (i in 1:5000){
  output[[i]] = sim_mu_p(mu)
}

sim_results = bind_rows(output) 
```

The code below repeats the above for Î¼={1,2,3,4,5,6}.

```{r}
sim_df = 
  expand_grid(
    n = 30,
    sigma = 5,
    mu = c(1,2,3,4,5,6),
    iter = 1:5000
  ) %>% 
  mutate(result = map(mu, sim_mu_p)) %>% 
  unnest(result)
```

I then make a plot showing the proportion of times the null was rejected (the power of the test) vs the true value of $\mu$.

```{r}
sim_df %>%  
  group_by(mu) %>% 
  summarise(
    total = n(),
    n_reject = sum(p.value<0.05),
    prop = n_reject/total
  ) %>% 
  ggplot(aes(x = mu, y = prop))+
  geom_point()+
  geom_path()+
  labs(
    title = "Power of the test vs true mu",
    x = "mu",
    y = "Proportion of times the null was rejected"
  )
```

From the plot, we can see that when the effect size increases, the power increases, but the increasing rate get smaller and tends to 0.

The plot below shows the average estimate of $\hat{\mu}$ and the average estimate of $\hat{\mu}$ only in samples for which the null was rejected vs the true $\mu$
 
```{r}
sim_df %>% 
  group_by(mu) %>% 
  summarise(
    avg_mu_hat_1 = mean(estimate)
  ) %>% left_join(
    sim_df %>% 
      filter(p.value<0.05) %>% 
      group_by(mu) %>% 
      summarise(
        avg_mu_hat_2 = mean(estimate)
  ), by = "mu"
  ) %>% 
  pivot_longer(
    2:3,
    names_to = "mu_hat",
    names_prefix = "avg_mu_hat_",
    values_to = "avg_estimate"
  ) %>% 
  ggplot(aes(y = avg_estimate, x = mu, color = mu_hat)) +
  geom_point()+
  geom_path()+
  labs(
    x = "True mu",
    y = "Average estimate of mu_hat"
  )
```

The sample average of $\hat{\mu}$ across tests for which the null is rejected approximately equal to the true value of $\mu$ only when effect size/power of the test is large. 

